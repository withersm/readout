{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example stuff from ali_offline_demod.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.formatter.useoffset'] = False\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "from scipy.signal import sawtooth, square, savgol_filter\n",
    "import pandas as pd\n",
    "import glob as gl\n",
    "import os\n",
    "import cmath\n",
    "\n",
    "from scipy.signal import sawtooth, square,find_peaks\n",
    "from scipy import spatial\n",
    "# import lambdafit as lf\n",
    "from scipy.interpolate import CubicSpline,interp1d\n",
    "import h5py\n",
    "\n",
    "from tqdm import tqdm as tqdm_terminal\n",
    "from tqdm.notebook import trange, tqdm_notebook\n",
    "from scipy.signal.windows import hann\n",
    "\n",
    "from scipy.fft import fft, ifft, fftfreq\n",
    "from copy import deepcopy\n",
    "from scipy.interpolate import CubicSpline, interp1d\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matt's current read_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for reading, processing, and demodulating real data\n",
    "def read_data(filename,channels='all',start_channel=0,stop_channel=1000):\n",
    "    if channels == 'all':\n",
    "        file = h5py.File(filename, 'r')\n",
    "        adc_i = np.array(file['time_ordered_data']['adc_i'])\n",
    "        adc_i = np.delete(adc_i, slice(0,22), 0)\n",
    "        adc_q = file['time_ordered_data']['adc_q']\n",
    "        adc_q = np.delete(adc_q, slice(0,22), 0)\n",
    "        t = np.array(file['time_ordered_data']['timestamp'])  \n",
    "    elif channels == 'some':\n",
    "        start_channel += 23 #eliminate the first 23 empty channels in hdf5 -> makes channel numbering match resonator numbering\n",
    "        stop_channel += 23 + 1 #eliminate the first 23 empty channels in hdf5 -> makes channel numbering match resonator numbering; +1 forces python to include the stop_channel\n",
    "        file = h5py.File(filename, 'r')\n",
    "        adc_i = np.array(file['time_ordered_data']['adc_i'][start_channel:stop_channel]) \n",
    "        adc_q = np.array(file['time_ordered_data']['adc_q'][start_channel:stop_channel]) \n",
    "        t = np.array(file['time_ordered_data']['timestamp'])  \n",
    "    \n",
    "    return t, adc_i, adc_q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/leayamashiro/AliCPT/ali_LY_git'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create file object\n",
    "file = h5py.File('../alicpt_data/data_files/ts_toneinit_fcenter_4250.0_20240506174818_t_20240506191017.hd5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 dictionary:  ['dimension', 'global_data', 'time_ordered_data']\n"
     ]
    }
   ],
   "source": [
    "# h5py.File acts like Python dictionary -- this means we can check keys: \n",
    "print('HDF5 dictionary: ', list(file.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension keys:  ['n_attenuators', 'n_fftbins', 'n_sample', 'n_tones']\n",
      "global data keys:  ['attenuator_settings', 'baseband_freqs', 'chan_number', 'chanmask', 'detector_beam_ampl', 'detector_delta_x', 'detector_delta_y', 'detector_dx_dy_elevation_angle', 'detector_pol', 'dfoverf_per_mK', 'ifslice_number', 'lo_freq', 'rfsoc_number', 'sample_rate', 'tile_number', 'tone_powers']\n",
      "time_data keys:  ['adc_i', 'adc_q', 'timestamp']\n"
     ]
    }
   ],
   "source": [
    "# based on that, there are three datasets in the file\n",
    "# we can examine each key in the set - create different objects: \n",
    "\n",
    "dimension = file['dimension']\n",
    "global_data = file['global_data']\n",
    "time_data = file['time_ordered_data']\n",
    "\n",
    "# figured out that these are 'Group' objects, not individual data sets\n",
    "# inspect them with same list function: \n",
    "\n",
    "print('dimension keys: ', list(dimension))\n",
    "print('global data keys: ', list(global_data))\n",
    "print('time_data keys: ', list(time_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(time_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay, now seeing that there are many pieces of information in this set – based on Matt's 'read_data' function, the relevant data to work on is the 'time_ordered_data'\n",
    "\n",
    "\n",
    "i am going to assume thus that i do not need to worry about the 'dimension' and 'global' objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time data properties --\n",
      "i: (1024, 293288) int32\n",
      "q: (1024, 293288) int32\n",
      "t: (293288,) float64\n"
     ]
    }
   ],
   "source": [
    "# organize time_data into its constituent parts for personal inspection: \n",
    "\n",
    "i_hd5 = time_data['adc_i']\n",
    "q_hd5 = time_data['adc_q']\n",
    "t_hd5 = time_data['timestamp']\n",
    "\n",
    "# inspect: \n",
    "\n",
    "print('time data properties --')\n",
    "print('i:', i_hd5.shape, i_hd5.dtype)\n",
    "print('q:', q_hd5.shape, q_hd5.dtype)\n",
    "print('t:', t_hd5.shape, t_hd5.dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time investigation: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1715047819.7287214"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so 1024 channels, \n",
    "\n",
    "t_hd5[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first look: \n",
    "\n",
    "# # Assuming time_data contains 'adc_i', 'adc_q', and 'timestamp'\n",
    "# adc_i = time_data['adc_i']  # Shape: (1024, 293288)\n",
    "# adc_q = time_data['adc_q']  # Shape: (1024, 293288)\n",
    "# timestamp = time_data['timestamp']  # Shape: (293288,)\n",
    "\n",
    "# # Flatten the 'adc_i' and 'adc_q' arrays to make them column vectors\n",
    "# # If we want to treat each sample in 'adc_i' and 'adc_q' as a separate entry, we can concatenate them\n",
    "# adc_i_flat = adc_i.flatten()  # Flattened shape: (1024 * 293288,)\n",
    "# adc_q_flat = adc_q.flatten()  # Flattened shape: (1024 * 293288,)\n",
    "\n",
    "# # Combine the flattened 'adc_i', 'adc_q', and 'timestamp' into a structured array or DataFrame\n",
    "# # We need to repeat the timestamp values to match the flattened data shape\n",
    "# timestamp_repeated = np.tile(timestamp, adc_i.shape[0])  # Repeat the timestamp for each of the 1024 samples\n",
    "\n",
    "# # Create the DataFrame\n",
    "# df = pd.DataFrame({\n",
    "#     'timestamp': timestamp_repeated,\n",
    "#     'adc_i': adc_i_flat,\n",
    "#     'adc_q': adc_q_flat\n",
    "# })\n",
    "\n",
    "# # View the first few rows of the DataFrame\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# based on Matt's function -- i = np.array(file['time_ordered_data']['adc_i'])\u001b[39;00m\n\u001b[1;32m      7\u001b[0m i[\u001b[38;5;241m20000\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([t_hd5, [i_hd5, q_hd5]])\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# so we need to have arrays with time stamp events corresponding to the i and q data, go back and use the file object\n",
    "\n",
    "i = np.array(t_hd5, i_hd5)\n",
    "q = np.array(t_hd5, q_hd5)\n",
    "\n",
    "# based on Matt's function -- i = np.array(file['time_ordered_data']['adc_i'])\n",
    "i[20000]\n",
    "\n",
    "data = np.array([t_hd5, [i_hd5, q_hd5]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,) (1,) (1,) (1,)\n"
     ]
    }
   ],
   "source": [
    "# wondering about 'dimension' object for chunking: \n",
    "\n",
    "n_atten = dimension['n_attenuators']\n",
    "n_fft = dimension['n_fftbins']\n",
    "n_samp = dimension['n_sample']\n",
    "n_tone = dimension['n_tones']\n",
    "\n",
    "print(n_atten.shape, n_fft.shape, n_samp.shape, n_tone.shape)\n",
    "\n",
    "# okay so not going to be impacted in the chunking because just characteristic data \n",
    "# if it's single-value data then why not call it 'global data'? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.71504782e+09, 1.71504782e+09, 1.71504782e+09, ...,\n",
       "       1.71504786e+09, 1.71504786e+09, 1.71504786e+09])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just looking \n",
    "\n",
    "t[0:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1715047922.1249328"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we have a set of 293288 events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m q\u001b[38;5;241m.\u001b[39mindex\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'index'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286.4140625"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "293288/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdriver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlibver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muserblock_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrdcc_nslots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrdcc_nbytes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrdcc_w0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfs_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfs_page_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpage_buf_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_meta_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_raw_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0malignment_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0malignment_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmeta_block_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m      Represents an HDF5 file.\n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "Create a new file object.\n",
      "\n",
      "See the h5py user guide for a detailed explanation of the options.\n",
      "\n",
      "name\n",
      "    Name of the file on disk, or file-like object.  Note: for files\n",
      "    created with the 'core' driver, HDF5 still requires this be\n",
      "    non-empty.\n",
      "mode\n",
      "    r        Readonly, file must exist (default)\n",
      "    r+       Read/write, file must exist\n",
      "    w        Create file, truncate if exists\n",
      "    w- or x  Create file, fail if exists\n",
      "    a        Read/write if exists, create otherwise\n",
      "driver\n",
      "    Name of the driver to use.  Legal values are None (default,\n",
      "    recommended), 'core', 'sec2', 'direct', 'stdio', 'mpio', 'ros3'.\n",
      "libver\n",
      "    Library version bounds.  Supported values: 'earliest', 'v108',\n",
      "    'v110', 'v112'  and 'latest'. The 'v108', 'v110' and 'v112'\n",
      "    options can only be specified with the HDF5 1.10.2 library or later.\n",
      "userblock_size\n",
      "    Desired size of user block.  Only allowed when creating a new\n",
      "    file (mode w, w- or x).\n",
      "swmr\n",
      "    Open the file in SWMR read mode. Only used when mode = 'r'.\n",
      "rdcc_nbytes\n",
      "    Total size of the dataset chunk cache in bytes. The default size\n",
      "    is 1024**2 (1 MiB) per dataset. Applies to all datasets unless individually changed.\n",
      "rdcc_w0\n",
      "    The chunk preemption policy for all datasets.  This must be\n",
      "    between 0 and 1 inclusive and indicates the weighting according to\n",
      "    which chunks which have been fully read or written are penalized\n",
      "    when determining which chunks to flush from cache.  A value of 0\n",
      "    means fully read or written chunks are treated no differently than\n",
      "    other chunks (the preemption is strictly LRU) while a value of 1\n",
      "    means fully read or written chunks are always preempted before\n",
      "    other chunks.  If your application only reads or writes data once,\n",
      "    this can be safely set to 1.  Otherwise, this should be set lower\n",
      "    depending on how often you re-read or re-write the same data.  The\n",
      "    default value is 0.75. Applies to all datasets unless individually changed.\n",
      "rdcc_nslots\n",
      "    The number of chunk slots in the raw data chunk cache for this\n",
      "    file. Increasing this value reduces the number of cache collisions,\n",
      "    but slightly increases the memory used. Due to the hashing\n",
      "    strategy, this value should ideally be a prime number. As a rule of\n",
      "    thumb, this value should be at least 10 times the number of chunks\n",
      "    that can fit in rdcc_nbytes bytes. For maximum performance, this\n",
      "    value should be set approximately 100 times that number of\n",
      "    chunks. The default value is 521. Applies to all datasets unless individually changed.\n",
      "track_order\n",
      "    Track dataset/group/attribute creation order under root group\n",
      "    if True. If None use global default h5.get_config().track_order.\n",
      "fs_strategy\n",
      "    The file space handling strategy to be used.  Only allowed when\n",
      "    creating a new file (mode w, w- or x).  Defined as:\n",
      "    \"fsm\"        FSM, Aggregators, VFD\n",
      "    \"page\"       Paged FSM, VFD\n",
      "    \"aggregate\"  Aggregators, VFD\n",
      "    \"none\"       VFD\n",
      "    If None use HDF5 defaults.\n",
      "fs_page_size\n",
      "    File space page size in bytes. Only used when fs_strategy=\"page\". If\n",
      "    None use the HDF5 default (4096 bytes).\n",
      "fs_persist\n",
      "    A boolean value to indicate whether free space should be persistent\n",
      "    or not.  Only allowed when creating a new file.  The default value\n",
      "    is False.\n",
      "fs_threshold\n",
      "    The smallest free-space section size that the free space manager\n",
      "    will track.  Only allowed when creating a new file.  The default\n",
      "    value is 1.\n",
      "page_buf_size\n",
      "    Page buffer size in bytes. Only allowed for HDF5 files created with\n",
      "    fs_strategy=\"page\". Must be a power of two value and greater or\n",
      "    equal than the file space page size when creating the file. It is\n",
      "    not used by default.\n",
      "min_meta_keep\n",
      "    Minimum percentage of metadata to keep in the page buffer before\n",
      "    allowing pages containing metadata to be evicted. Applicable only if\n",
      "    page_buf_size is set. Default value is zero.\n",
      "min_raw_keep\n",
      "    Minimum percentage of raw data to keep in the page buffer before\n",
      "    allowing pages containing raw data to be evicted. Applicable only if\n",
      "    page_buf_size is set. Default value is zero.\n",
      "locking\n",
      "    The file locking behavior. Defined as:\n",
      "\n",
      "    - False (or \"false\") --  Disable file locking\n",
      "    - True (or \"true\")   --  Enable file locking\n",
      "    - \"best-effort\"      --  Enable file locking but ignore some errors\n",
      "    - None               --  Use HDF5 defaults\n",
      "\n",
      "    .. warning::\n",
      "\n",
      "        The HDF5_USE_FILE_LOCKING environment variable can override\n",
      "        this parameter.\n",
      "\n",
      "    Only available with HDF5 >= 1.12.1 or 1.10.x >= 1.10.7.\n",
      "\n",
      "alignment_threshold\n",
      "    Together with ``alignment_interval``, this property ensures that\n",
      "    any file object greater than or equal in size to the alignement\n",
      "    threshold (in bytes) will be aligned on an address which is a\n",
      "    multiple of alignment interval.\n",
      "\n",
      "alignment_interval\n",
      "    This property should be used in conjunction with\n",
      "    ``alignment_threshold``. See the description above. For more\n",
      "    details, see\n",
      "    https://portal.hdfgroup.org/display/HDF5/H5P_SET_ALIGNMENT\n",
      "\n",
      "meta_block_size\n",
      "    Set the current minimum size, in bytes, of new metadata block allocations.\n",
      "    See https://portal.hdfgroup.org/display/HDF5/H5P_SET_META_BLOCK_SIZE\n",
      "\n",
      "Additional keywords\n",
      "    Passed on to the selected file driver.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/lib/python3.11/site-packages/h5py/_hl/files.py\n",
      "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "h5py.File?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM PAST: (can utilize for non-pandas numpy setup)\n",
    "\n",
    "# make length object, empty arrays\n",
    "len_testdata = len(testdata) \n",
    "datalist = [] \n",
    "testTime = []\n",
    "dataCount = []\n",
    "\n",
    "# iterate through all 'testdata' object and pull out correlation\n",
    "# info, time info, and count info \n",
    "for n in range(len_testdata):\n",
    "    test_dataFiles = np.load(testdata[n])\n",
    "    datalist.append(test_dataFiles[\"data\"]) # Append data\n",
    "    testTime.append(test_dataFiles[\"time\"]) # Append times\n",
    "    dataCount.append(test_dataFiles[\"count\"]) # Append counts \n",
    "# concatenate data with vertical stack\n",
    "test_data = np.concatenate(datalist, axis=0)  \n",
    "data_times = np.concatenate(testTime, axis=0)\n",
    "data_counts = np.concatenate(dataCount, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# make a combined data object\n",
    "data = [{\"data\": test_data,\n",
    "        \"time\": data_times,\n",
    "        \"count\": data_counts}]\n",
    "# OR make an ASCII table with it??\n",
    "from astropy.table import Table\n",
    "from astropy.io import ascii\n",
    "# Okay too hard because data objects have multiple dimensions lol "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
